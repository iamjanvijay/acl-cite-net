@proceedings{woah-2021-online,
    title = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    editor = "Mostafazadeh Davani, Aida  and
      Kiela, Douwe  and
      Lambert, Mathias  and
      Vidgen, Bertie  and
      Prabhakaran, Vinodkumar  and
      Waseem, Zeerak",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.0",
}
@inproceedings{singh-li-2021-exploiting,
    title = "Exploiting Auxiliary Data for Offensive Language Detection with Bidirectional Transformers",
    author = "Singh, Sumer  and
      Li, Sheng",
    booktitle = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.1",
    doi = "10.18653/v1/2021.woah-1.1",
    pages = "1--5",
    abstract = "Offensive language detection (OLD) has received increasing attention due to its societal impact. Recent work shows that bidirectional transformer based methods obtain impressive performance on OLD. However, such methods usually rely on large-scale well-labeled OLD datasets for model training. To address the issue of data/label scarcity in OLD, in this paper, we propose a simple yet effective domain adaptation approach to train bidirectional transformers. Our approach introduces domain adaptation (DA) training procedures to ALBERT, such that it can effectively exploit auxiliary data from source domains to improve the OLD performance in a target domain. Experimental results on benchmark datasets show that our approach, ALBERT (DA), obtains the state-of-the-art performance in most cases. Particularly, our approach significantly benefits underrepresented and under-performing classes, with a significant improvement over ALBERT.",
}
@inproceedings{hahn-etal-2021-modeling,
    title = "Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces",
    author = "Hahn, Vanessa  and
      Ruiter, Dana  and
      Kleinbauer, Thomas  and
      Klakow, Dietrich",
    booktitle = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.2",
    doi = "10.18653/v1/2021.woah-1.2",
    pages = "6--16",
    abstract = "Hate speech and profanity detection suffer from data sparsity, especially for languages other than English, due to the subjective nature of the tasks and the resulting annotation incompatibility of existing corpora. In this study, we identify profane subspaces in word and sentence representations and explore their generalization capability on a variety of similar and distant target tasks in a zero-shot setting. This is done monolingually (German) and cross-lingually to closely-related (English), distantly-related (French) and non-related (Arabic) tasks. We observe that, on both similar and distant target tasks and across all languages, the subspace-based representations transfer more effectively than standard BERT representations in the zero-shot setting, with improvements between F1 +10.9 and F1 +42.9 over the baselines across all tested monolingual and cross-lingual scenarios.",
}
@inproceedings{caselli-etal-2021-hatebert,
    title = "{H}ate{BERT}: Retraining {BERT} for Abusive Language Detection in {E}nglish",
    author = "Caselli, Tommaso  and
      Basile, Valerio  and
      Mitrovi{\'c}, Jelena  and
      Granitzer, Michael",
    booktitle = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.3",
    doi = "10.18653/v1/2021.woah-1.3",
    pages = "17--25",
    abstract = "We introduce HateBERT, a re-trained BERT model for abusive language detection in English. The model was trained on RAL-E, a large-scale dataset of Reddit comments in English from communities banned for being offensive, abusive, or hateful that we have curated and made available to the public. We present the results of a detailed comparison between a general pre-trained language model and the retrained version on three English datasets for offensive, abusive language and hate speech detection tasks. In all datasets, HateBERT outperforms the corresponding general BERT model. We also discuss a battery of experiments comparing the portability of the fine-tuned models across the datasets, suggesting that portability is affected by compatibility of the annotated phenomena.",
}
@inproceedings{kirk-etal-2021-memes,
    title = "Memes in the Wild: Assessing the Generalizability of the Hateful Memes Challenge Dataset",
    author = "Kirk, Hannah  and
      Jun, Yennie  and
      Rauba, Paulius  and
      Wachtel, Gal  and
      Li, Ruining  and
      Bai, Xingjian  and
      Broestl, Noah  and
      Doff-Sotta, Martin  and
      Shtedritski, Aleksandar  and
      Asano, Yuki M",
    booktitle = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.4",
    doi = "10.18653/v1/2021.woah-1.4",
    pages = "26--35",
    abstract = "Hateful memes pose a unique challenge for current machine learning systems because their message is derived from both text- and visual-modalities. To this effect, Facebook released the Hateful Memes Challenge, a dataset of memes with pre-extracted text captions, but it is unclear whether these synthetic examples generalize to {`}memes in the wild{'}. In this paper, we collect hateful and non-hateful memes from Pinterest to evaluate out-of-sample performance on models pre-trained on the Facebook dataset. We find that {`}memes in the wild{'} differ in two key aspects: 1) Captions must be extracted via OCR, injecting noise and diminishing performance of multimodal models, and 2) Memes are more diverse than {`}traditional memes{'}, including screenshots of conversations or text on a plain background. This paper thus serves as a reality-check for the current benchmark of hateful meme detection and its applicability for detecting real world hate.",
}
@inproceedings{kivlichan-etal-2021-measuring,
    title = "Measuring and Improving Model-Moderator Collaboration using Uncertainty Estimation",
    author = "Kivlichan, Ian  and
      Lin, Zi  and
      Liu, Jeremiah  and
      Vasserman, Lucy",
    booktitle = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.5",
    doi = "10.18653/v1/2021.woah-1.5",
    pages = "36--53",
    abstract = "Content moderation is often performed by a collaboration between humans and machine learning models. However, it is not well understood how to design the collaborative process so as to maximize the combined moderator-model system performance. This work presents a rigorous study of this problem, focusing on an approach that incorporates model uncertainty into the collaborative process. First, we introduce principled metrics to describe the performance of the collaborative system under capacity constraints on the human moderator, quantifying how efficiently the combined system utilizes human decisions. Using these metrics, we conduct a large benchmark study evaluating the performance of state-of-the-art uncertainty models under different collaborative review strategies. We find that an uncertainty-based strategy consistently outperforms the widely used strategy based on toxicity scores, and moreover that the choice of review strategy drastically changes the overall system performance. Our results demonstrate the importance of rigorous metrics for understanding and developing effective moderator-model systems for content moderation, as well as the utility of uncertainty estimation in this domain.",
}
@inproceedings{caselli-etal-2021-dalc,
    title = "{DALC}: the {D}utch Abusive Language Corpus",
    author = "Caselli, Tommaso  and
      Schelhaas, Arjan  and
      Weultjes, Marieke  and
      Leistra, Folkert  and
      van der Veen, Hylke  and
      Timmerman, Gerben  and
      Nissim, Malvina",
    booktitle = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.6",
    doi = "10.18653/v1/2021.woah-1.6",
    pages = "54--66",
    abstract = "As socially unacceptable language become pervasive in social media platforms, the need for automatic content moderation become more pressing. This contribution introduces the Dutch Abusive Language Corpus (DALC v1.0), a new dataset with tweets manually an- notated for abusive language. The resource ad- dress a gap in language resources for Dutch and adopts a multi-layer annotation scheme modeling the explicitness and the target of the abusive messages. Baselines experiments on all annotation layers have been conducted, achieving a macro F1 score of 0.748 for binary classification of the explicitness layer and .489 for target classification.",
}
@inproceedings{niraula-etal-2021-offensive,
    title = "Offensive Language Detection in {N}epali Social Media",
    author = "Niraula, Nobal B.  and
      Dulal, Saurab  and
      Koirala, Diwa",
    booktitle = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.7",
    doi = "10.18653/v1/2021.woah-1.7",
    pages = "67--75",
    abstract = "Social media texts such as blog posts, comments, and tweets often contain offensive languages including racial hate speech comments, personal attacks, and sexual harassment. Detecting inappropriate use of language is, therefore, of utmost importance for the safety of the users as well as for suppressing hateful conduct and aggression. Existing approaches to this problem are mostly available for resource-rich languages such as English and German. In this paper, we characterize the offensive language in Nepali, a low-resource language, highlighting the challenges that need to be addressed for processing Nepali social media text. We also present experiments for detecting offensive language using supervised machine learning. Besides contributing the first baseline approaches of detecting offensive language in Nepali, we also release human annotated data sets to encourage future research on this crucial topic.",
}
@inproceedings{fortuna-etal-2021-min,
    title = "{MIN}{\_}{PT}: An {E}uropean {P}ortuguese Lexicon for Minorities Related Terms",
    author = "Fortuna, Paula  and
      Cortez, Vanessa  and
      Sozinho Ramalho, Miguel  and
      P{\'e}rez-Mayos, Laura",
    booktitle = "Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.woah-1.8",
    doi = "10.18653/v1/2021.woah-1.8",
    pages = "76--80",
    abstract = "Hate speech-related lexicons have been proved to be useful for many tasks such as data collection and classification. However, existing Portuguese lexicons do not distinguish between European and Brazilian Portuguese, and do not include neutral terms that are potentially useful to detect a broader spectrum of content referring to minorities. In this work, we present MIN{\_}PT, a new European Portuguese Lexicon for Minorities-Related Terms specifically designed to tackle the limitations of existing resources. We describe the data collection and annotation process, discuss the limitation and ethical concerns, and prove the utility of the resource by applying it to a use case for the Portuguese 2021 presidential elections.",
}
